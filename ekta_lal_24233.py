# -*- coding: utf-8 -*-
"""Ekta Lal-24233

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M9TRFepTlTn76xWUW7GZgVhTeUsa8MwL
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score
from yellowbrick.classifier import ConfusionMatrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

df=pd.read_csv('/content/insurance_claims.csv')

df.count()

df.shape

df.head()

df.tail()

#filling any null values in the dataset
print(df.isnull().sum())
df.fillna(0,inplace=True)
available_cols = [col for col in ['PolicyholderOccupation','FirstPartyVehicleType','ClaimInvolvedCovers','ClaimCause','ConnectionBetweenParties']]
existing_cols = [col for col in available_cols if col in df.columns]
if existing_cols:
  df = pd.get_dummies(df, columns=existing_cols, drop_first=True)

df.info()

plt.figure (figsize=(8,11))

#Graph for number of policies by policy holders
plt.subplot(2,2,1)
sns.countplot(data=df, x='NumberOfPoliciesOfPolicyholder')
plt.title('Policies')

#Graph for Policy Holder Age
plt.subplot(3,2,2)
sns.countplot(data=df, x='PolicyHolderAge')
plt.title('Policy Holder Age')

#Graph for Easiness to stage
plt.subplot(3,2,3)
sns.countplot(data=df, x='EasinessToStage')
plt.title('Easiness to stage')

#Graph for Fraud
plt.subplot(3,2,4)
sns.countplot(data=df, x='Fraud')
plt.title('Fraud')

#drop irrelevant columns
columns_to_drop = ['FirstPartyVehicleNumber', 'RefernceID', 'ThirdPartyVehicleNumber', 'InsurerNotes','PolicyHolderNumber','LossDate','FisrtPolicySubscriptionDate']
df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)

#Data Split
#independent
X = df.drop(columns=['Fraud'])

#dependent
Y = df['Fraud']

#convert categorical columns to numerical columns
X = pd.get_dummies(X, drop_first=True)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, Y_resampled = smote.fit_resample(X, Y)

# Train-Test Split (20% test, 80% train)

X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X_resampled, Y_resampled, test_size=0.2, random_state=42)

#Normalizing the data
scaler = StandardScaler()

#Fit and transform training data
X_train_scaled = scaler.fit_transform(X_train_data)

#Transform Test data
X_test_scaled = scaler.transform(X_test_data)

#create logistic regression model
#took help from chatgpt
logistic_regression = LogisticRegression(max_iter=100)
logistic_regression.fit(X_train_scaled, Y_train_data)
y_pred = logistic_regression.predict(X_test_scaled)
y_prob = logistic_regression.predict_proba(X_test_scaled)[:,1]

#model's performance
Acc = accuracy_score(Y_test_data, y_pred)
Confusion_Matrix = confusion_matrix(Y_test_data, y_pred)
Class_Report = classification_report(Y_test_data, y_pred)
AUC_ROC = roc_auc_score(Y_test_data, y_prob)

#print performance metrics
print(f"Model Accuracy: {Acc}")
print(f"Confusion Matrix:\n{Confusion_Matrix}")
print(f"Classification Report:\n{Class_Report}")
print(f"AUC-ROC Score: {AUC_ROC:.4f}")

Confusion_Matrix = confusion_matrix(Y_test_data, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(Confusion_Matrix, annot=True, fmt='d', cmap='Purples', linewidths=0.5, linecolor='black')
plt.xlabel('Predicted Frauds')
plt.ylabel('Actual Frauds')
plt.title('Confusion Matrix')
plt.show()

"""**Evaluation:**

The model has an AUC-ROC of exactly 1.0 and an accuracy rate of
99.39%, which indicates that the model can accurately distinguish between fraud insurace claims and non-fraud insurance claims most of the times.

The model has only 24 False Positives out of 3960, which means model predicted non-fraud cases as fraud only 24 times.

The model has 0 False Negatives out of 3960, which means none of the fraud cases were missed by the model.

The model has high Recall and Precision, which indicated that the model is balanced and is not only almost accurately detecting frauds but also avoiding missing any fraud cases.

**Insights and Recommendations:**

The False Positive of 24 can be slightly porblematic because it means few of the actual insurance claims are predicted as Fraud cases by the model. This incorrect prediction can inconvenience the client.

The model has AUC-ROC of 1.0, which indicates overfitting data, which means the model works near to perfect on trained data but might not work in real-life scenarios.

Encoding categorical variables to numeric data helps model understand categorical relationship between variables.

Since fraud cases were rare in this case, using SMOTE helped with balancing data and prevented the model from being biased towards non-fraud cases by creating synthetic fraud cases.

Standard Scaler helped with improving model stability by making sure each feature had an equal impact on fraud detection and not dominating others.
"""